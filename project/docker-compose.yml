# docker-compose.yml

version: "3.8"

services:
  backend:
    build:
      context: ./backend
    container_name: backend
    depends_on:
      - qdrant
    ports:
      - "5001:5001"
    volumes:
      - ./backend:/app  # Optional for development
    environment:
      - ENV=production

  ui:
    build:
      context: ./ui
    container_name: ui
    ports:
      - "7860:7860"
    depends_on:
      - backend
      - qdrant
      - ollama
    volumes:
      - ./ui:/app  # Optional for development

  qdrant:
    image: qdrant/qdrant
    container_name: qdrant
    ports:
      - "6333:6333"  # Expose Qdrant's API
    volumes:
      - qdrant_data:/qdrant/storage  # Persist Qdrant data

  ollama:
    image: ollama/ollama
    container_name: ollama
    environment:
      - OLLAMA_MODELS=/root/.ollama/models  # Set the environment variable to the desired directory inside the container
    volumes:
      - ollama_models:/root/.ollama/models  # Mount the Docker volume to the directory specified by OLLAMA_MODELS
    ports:
      - "11434:11434"
    depends_on:
      - backend
    command: >
      bash -c "ollama install llama3.1:latest && exec ollama" # Installs llama3.1:latest and runs ollama, Add all LLM that you are planning to use and update the name in GradioUI

volumes:
  qdrant_data:
  ollama_models:
    driver: local
